{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from TextDataset import TextDataset\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbaedc8618840479f229d4adf632631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9f60f6f6e546a7ab2cd1dfafe9bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 140.4180, Accuracy: 0.56, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 129.0576, Accuracy: 0.70, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 103.3430, Accuracy: 0.72, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 92.9094, Accuracy: 0.77, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 73.2468, Accuracy: 0.76, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 61.1320, Accuracy: 0.74, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 42.4121, Accuracy: 0.77, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 31.1416, Accuracy: 0.76, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 24.6619, Accuracy: 0.74, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 15.6909, Accuracy: 0.76, Learning Rate: 0.00000444\n",
      "Epoch 11/50, Loss: 10.0567, Accuracy: 0.78, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 8.3988, Accuracy: 0.77, Learning Rate: 0.00000422\n",
      "Epoch 13/50, Loss: 5.3452, Accuracy: 0.80, Learning Rate: 0.00000411\n",
      "Epoch 14/50, Loss: 4.2722, Accuracy: 0.81, Learning Rate: 0.00000400\n",
      "Epoch 15/50, Loss: 0.1609, Accuracy: 0.79, Learning Rate: 0.00000389\n",
      "Epoch 16/50, Loss: 4.5160, Accuracy: 0.79, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 17/50, Loss: 1.7469, Accuracy: 0.79, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 18/50, Loss: 2.3842, Accuracy: 0.80, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 19/50, Loss: 3.9012, Accuracy: 0.80, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 20/50, Loss: 0.0374, Accuracy: 0.79, Learning Rate: 0.00000333\n",
      "Epoch 21/50, Loss: 0.0227, Accuracy: 0.79, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 0.0196, Accuracy: 0.79, Learning Rate: 0.00000311\n",
      "Epoch 23/50, Loss: 0.0173, Accuracy: 0.79, Learning Rate: 0.00000300\n",
      "Epoch 24/50, Loss: 0.0155, Accuracy: 0.79, Learning Rate: 0.00000289\n",
      "Epoch 25/50, Loss: 0.0142, Accuracy: 0.80, Learning Rate: 0.00000278\n",
      "Epoch 26/50, Loss: 0.0363, Accuracy: 0.77, Learning Rate: 0.00000267\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 27/50, Loss: 3.4258, Accuracy: 0.74, Learning Rate: 0.00000256\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 28/50, Loss: 4.5592, Accuracy: 0.79, Learning Rate: 0.00000244\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 29/50, Loss: 1.9453, Accuracy: 0.81, Learning Rate: 0.00000233\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 30/50, Loss: 2.0444, Accuracy: 0.81, Learning Rate: 0.00000222\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca7ed0e0c7847d1b5be0170a8a39366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c023759b084245d4b299a665ad7c32c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 206.9853, Accuracy: 0.63, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 190.8162, Accuracy: 0.74, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 150.6551, Accuracy: 0.77, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 131.0740, Accuracy: 0.81, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 115.3223, Accuracy: 0.76, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 87.1561, Accuracy: 0.79, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 67.2675, Accuracy: 0.77, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 31.8927, Accuracy: 0.81, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 22.6614, Accuracy: 0.79, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 15.1012, Accuracy: 0.83, Learning Rate: 0.00000444\n",
      "Epoch 11/50, Loss: 9.3496, Accuracy: 0.83, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 10.4503, Accuracy: 0.83, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 4.4448, Accuracy: 0.80, Learning Rate: 0.00000411\n",
      "Epoch 14/50, Loss: 3.7298, Accuracy: 0.78, Learning Rate: 0.00000400\n",
      "Epoch 15/50, Loss: 9.5784, Accuracy: 0.80, Learning Rate: 0.00000389\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 16/50, Loss: 9.9854, Accuracy: 0.78, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 17/50, Loss: 7.1024, Accuracy: 0.77, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 18/50, Loss: 6.3630, Accuracy: 0.81, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 19/50, Loss: 6.8807, Accuracy: 0.81, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8cebdf38924a29988d4aa0859d836d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f7b38dfd5d46c18ea69a59ce756a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 277.1521, Accuracy: 0.71, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 242.9750, Accuracy: 0.69, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 178.1995, Accuracy: 0.74, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 133.0205, Accuracy: 0.78, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 83.3438, Accuracy: 0.81, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 46.0899, Accuracy: 0.78, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 21.3974, Accuracy: 0.79, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 5.4834, Accuracy: 0.80, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 1.9716, Accuracy: 0.79, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 3.8953, Accuracy: 0.78, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 11/50, Loss: 0.0378, Accuracy: 0.79, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 8.9805, Accuracy: 0.81, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 3.5534, Accuracy: 0.80, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 14/50, Loss: 14.8527, Accuracy: 0.76, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 15/50, Loss: 13.2351, Accuracy: 0.77, Learning Rate: 0.00000389\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 16/50, Loss: 12.1377, Accuracy: 0.78, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d026da12a3d34853816f3d51027cdcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8520528865064d1e9d9a7c0858eaf8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 341.9149, Accuracy: 0.69, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 283.4325, Accuracy: 0.76, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 201.7199, Accuracy: 0.77, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 148.3268, Accuracy: 0.78, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 90.7507, Accuracy: 0.80, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 43.3521, Accuracy: 0.81, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 12.7130, Accuracy: 0.81, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 10.8235, Accuracy: 0.84, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 9.7582, Accuracy: 0.76, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 14.8208, Accuracy: 0.81, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 11/50, Loss: 2.4198, Accuracy: 0.73, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 16.9884, Accuracy: 0.77, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 3.4877, Accuracy: 0.80, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 14/50, Loss: 6.7032, Accuracy: 0.80, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 15/50, Loss: 12.8630, Accuracy: 0.79, Learning Rate: 0.00000389\n",
      "No improvement. Patience counter: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Loss: 17.5620, Accuracy: 0.79, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78121557ff764275a875cf05de716f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c22184aa154cfe9135a251d3e9ce41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 415.1383, Accuracy: 0.69, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 314.7491, Accuracy: 0.73, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 215.3113, Accuracy: 0.72, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 154.2976, Accuracy: 0.76, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 61.5541, Accuracy: 0.73, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 37.7574, Accuracy: 0.78, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 20.5580, Accuracy: 0.80, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 3.2556, Accuracy: 0.74, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 19.0255, Accuracy: 0.79, Learning Rate: 0.00000456\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 10/50, Loss: 3.5600, Accuracy: 0.78, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 11/50, Loss: 7.1690, Accuracy: 0.79, Learning Rate: 0.00000433\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 12/50, Loss: 1.6962, Accuracy: 0.80, Learning Rate: 0.00000422\n",
      "Epoch 13/50, Loss: 2.5640, Accuracy: 0.78, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 14/50, Loss: 15.0304, Accuracy: 0.78, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 15/50, Loss: 16.9451, Accuracy: 0.77, Learning Rate: 0.00000389\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 16/50, Loss: 0.4994, Accuracy: 0.77, Learning Rate: 0.00000378\n",
      "Epoch 17/50, Loss: 4.1564, Accuracy: 0.80, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 18/50, Loss: 8.4726, Accuracy: 0.72, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 19/50, Loss: 0.0054, Accuracy: 0.78, Learning Rate: 0.00000344\n",
      "Epoch 20/50, Loss: 4.3120, Accuracy: 0.77, Learning Rate: 0.00000333\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 21/50, Loss: 0.0033, Accuracy: 0.78, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 2.6032, Accuracy: 0.76, Learning Rate: 0.00000311\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 23/50, Loss: 5.6548, Accuracy: 0.77, Learning Rate: 0.00000300\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 24/50, Loss: 0.0012, Accuracy: 0.78, Learning Rate: 0.00000289\n",
      "Epoch 25/50, Loss: 7.0512, Accuracy: 0.77, Learning Rate: 0.00000278\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 26/50, Loss: 1.1386, Accuracy: 0.79, Learning Rate: 0.00000267\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 27/50, Loss: 4.4455, Accuracy: 0.76, Learning Rate: 0.00000256\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 28/50, Loss: 4.3186, Accuracy: 0.72, Learning Rate: 0.00000244\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 29/50, Loss: 3.0668, Accuracy: 0.73, Learning Rate: 0.00000233\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460900200b194704ba772719dca647d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214628f0192e430f922f3b580f2468a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 476.0063, Accuracy: 0.66, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 374.4086, Accuracy: 0.73, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 282.7456, Accuracy: 0.78, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 201.7153, Accuracy: 0.77, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 84.7679, Accuracy: 0.78, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 36.1207, Accuracy: 0.76, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 37.8837, Accuracy: 0.73, Learning Rate: 0.00000478\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 8/50, Loss: 13.4657, Accuracy: 0.78, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 11.2674, Accuracy: 0.79, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 19.4262, Accuracy: 0.76, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 11/50, Loss: 15.2733, Accuracy: 0.73, Learning Rate: 0.00000433\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 12/50, Loss: 12.2892, Accuracy: 0.67, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 13/50, Loss: 14.2146, Accuracy: 0.78, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 14/50, Loss: 14.8041, Accuracy: 0.80, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acff58af81c649e88546e679bc615dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ad0bc624cd47778d5974501a749466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 539.2783, Accuracy: 0.79, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 383.5781, Accuracy: 0.77, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 246.4083, Accuracy: 0.77, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 133.8383, Accuracy: 0.73, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 64.3514, Accuracy: 0.78, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 32.1910, Accuracy: 0.76, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 17.9336, Accuracy: 0.73, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 13.1971, Accuracy: 0.77, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 16.7148, Accuracy: 0.77, Learning Rate: 0.00000456\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 10/50, Loss: 23.9973, Accuracy: 0.76, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 11/50, Loss: 0.9799, Accuracy: 0.78, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 17.5757, Accuracy: 0.78, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 12.7256, Accuracy: 0.70, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 14/50, Loss: 2.5235, Accuracy: 0.77, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 15/50, Loss: 0.0059, Accuracy: 0.77, Learning Rate: 0.00000389\n",
      "Epoch 16/50, Loss: 18.1924, Accuracy: 0.79, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 17/50, Loss: 13.5825, Accuracy: 0.76, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 18/50, Loss: 0.3242, Accuracy: 0.77, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 19/50, Loss: 5.5977, Accuracy: 0.78, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 20/50, Loss: 9.1149, Accuracy: 0.76, Learning Rate: 0.00000333\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227eb2ae83494f82817d6c9324bb5cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67bfc12bd96452d8209f452ac8d3fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 594.5876, Accuracy: 0.73, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 409.7688, Accuracy: 0.70, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 231.5440, Accuracy: 0.73, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 126.3211, Accuracy: 0.74, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 72.0467, Accuracy: 0.73, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 39.2175, Accuracy: 0.74, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 20.4795, Accuracy: 0.76, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 8.4884, Accuracy: 0.78, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 11.7885, Accuracy: 0.73, Learning Rate: 0.00000456\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 10/50, Loss: 13.6621, Accuracy: 0.73, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 11/50, Loss: 6.9855, Accuracy: 0.78, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 13.1085, Accuracy: 0.79, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 26.7054, Accuracy: 0.76, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 14/50, Loss: 8.5624, Accuracy: 0.72, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 15/50, Loss: 4.9537, Accuracy: 0.72, Learning Rate: 0.00000389\n",
      "Epoch 16/50, Loss: 0.7060, Accuracy: 0.74, Learning Rate: 0.00000378\n",
      "Epoch 17/50, Loss: 11.3960, Accuracy: 0.74, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 18/50, Loss: 1.5420, Accuracy: 0.74, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 19/50, Loss: 20.7837, Accuracy: 0.73, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 20/50, Loss: 0.5799, Accuracy: 0.76, Learning Rate: 0.00000333\n",
      "Epoch 21/50, Loss: 0.0017, Accuracy: 0.77, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 13.0727, Accuracy: 0.76, Learning Rate: 0.00000311\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 23/50, Loss: 6.8888, Accuracy: 0.76, Learning Rate: 0.00000300\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 24/50, Loss: 1.6239, Accuracy: 0.74, Learning Rate: 0.00000289\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 25/50, Loss: 0.8089, Accuracy: 0.77, Learning Rate: 0.00000278\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 26/50, Loss: 0.0008, Accuracy: 0.77, Learning Rate: 0.00000267\n",
      "Epoch 27/50, Loss: 0.0006, Accuracy: 0.78, Learning Rate: 0.00000256\n",
      "Epoch 28/50, Loss: 0.0005, Accuracy: 0.78, Learning Rate: 0.00000244\n",
      "Epoch 29/50, Loss: 0.0004, Accuracy: 0.79, Learning Rate: 0.00000233\n",
      "Epoch 30/50, Loss: 14.4244, Accuracy: 0.74, Learning Rate: 0.00000222\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 31/50, Loss: 0.0007, Accuracy: 0.76, Learning Rate: 0.00000211\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 32/50, Loss: 0.0314, Accuracy: 0.77, Learning Rate: 0.00000200\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 33/50, Loss: 0.0005, Accuracy: 0.77, Learning Rate: 0.00000189\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 34/50, Loss: 3.8595, Accuracy: 0.76, Learning Rate: 0.00000178\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453466c87f564213b0bd2284eb8e61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d707557d14d8438393057fb94566e104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 647.9188, Accuracy: 0.76, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 446.0151, Accuracy: 0.77, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 295.4347, Accuracy: 0.77, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 146.9892, Accuracy: 0.76, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 54.9712, Accuracy: 0.79, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 46.4594, Accuracy: 0.79, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 14.2024, Accuracy: 0.78, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 18.6727, Accuracy: 0.83, Learning Rate: 0.00000467\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 9/50, Loss: 24.4601, Accuracy: 0.79, Learning Rate: 0.00000456\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 10/50, Loss: 7.4255, Accuracy: 0.79, Learning Rate: 0.00000444\n",
      "Epoch 11/50, Loss: 10.9906, Accuracy: 0.73, Learning Rate: 0.00000433\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 12/50, Loss: 12.0920, Accuracy: 0.74, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 13/50, Loss: 20.3540, Accuracy: 0.78, Learning Rate: 0.00000411\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 14/50, Loss: 13.1420, Accuracy: 0.76, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 15/50, Loss: 6.4849, Accuracy: 0.77, Learning Rate: 0.00000389\n",
      "Epoch 16/50, Loss: 0.0032, Accuracy: 0.78, Learning Rate: 0.00000378\n",
      "Epoch 17/50, Loss: 0.0020, Accuracy: 0.76, Learning Rate: 0.00000367\n",
      "Epoch 18/50, Loss: 17.5266, Accuracy: 0.73, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 19/50, Loss: 6.2528, Accuracy: 0.80, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 20/50, Loss: 4.9681, Accuracy: 0.83, Learning Rate: 0.00000333\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 21/50, Loss: 0.0014, Accuracy: 0.74, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 0.0009, Accuracy: 0.77, Learning Rate: 0.00000311\n",
      "Epoch 23/50, Loss: 2.5582, Accuracy: 0.76, Learning Rate: 0.00000300\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 24/50, Loss: 18.6136, Accuracy: 0.73, Learning Rate: 0.00000289\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 25/50, Loss: 0.0389, Accuracy: 0.73, Learning Rate: 0.00000278\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 26/50, Loss: 7.5628, Accuracy: 0.76, Learning Rate: 0.00000267\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 27/50, Loss: 0.0010, Accuracy: 0.74, Learning Rate: 0.00000256\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n",
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400, 3) (86, 5)\n",
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ae0daddfb34e5ba5dc69e203db48ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286914a310f24b2e939da9d917c88169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 695.9362, Accuracy: 0.76, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 468.9928, Accuracy: 0.76, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 293.2599, Accuracy: 0.76, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 109.4990, Accuracy: 0.73, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 61.7373, Accuracy: 0.74, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 35.4021, Accuracy: 0.80, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 33.9111, Accuracy: 0.80, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 23.1958, Accuracy: 0.78, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 28.3301, Accuracy: 0.77, Learning Rate: 0.00000456\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 10/50, Loss: 25.3678, Accuracy: 0.77, Learning Rate: 0.00000444\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 11/50, Loss: 7.7299, Accuracy: 0.83, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 21.0097, Accuracy: 0.77, Learning Rate: 0.00000422\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 13/50, Loss: 4.8573, Accuracy: 0.74, Learning Rate: 0.00000411\n",
      "Epoch 14/50, Loss: 7.6384, Accuracy: 0.76, Learning Rate: 0.00000400\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 15/50, Loss: 7.8962, Accuracy: 0.73, Learning Rate: 0.00000389\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 16/50, Loss: 7.6323, Accuracy: 0.77, Learning Rate: 0.00000378\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 17/50, Loss: 0.0021, Accuracy: 0.78, Learning Rate: 0.00000367\n",
      "Epoch 18/50, Loss: 17.1164, Accuracy: 0.74, Learning Rate: 0.00000356\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 19/50, Loss: 12.2323, Accuracy: 0.77, Learning Rate: 0.00000344\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 20/50, Loss: 9.2140, Accuracy: 0.77, Learning Rate: 0.00000333\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 21/50, Loss: 0.0020, Accuracy: 0.78, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 0.0016, Accuracy: 0.78, Learning Rate: 0.00000311\n",
      "Epoch 23/50, Loss: 0.0009, Accuracy: 0.78, Learning Rate: 0.00000300\n",
      "Epoch 24/50, Loss: 0.0006, Accuracy: 0.78, Learning Rate: 0.00000289\n",
      "Epoch 25/50, Loss: 0.0004, Accuracy: 0.78, Learning Rate: 0.00000278\n",
      "Epoch 26/50, Loss: 0.0003, Accuracy: 0.78, Learning Rate: 0.00000267\n",
      "Epoch 27/50, Loss: 0.0002, Accuracy: 0.78, Learning Rate: 0.00000256\n",
      "Epoch 28/50, Loss: 0.0001, Accuracy: 0.78, Learning Rate: 0.00000244\n",
      "Epoch 29/50, Loss: 0.0001, Accuracy: 0.78, Learning Rate: 0.00000233\n",
      "Epoch 30/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000222\n",
      "Epoch 31/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000211\n",
      "Epoch 32/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000200\n",
      "Epoch 33/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000189\n",
      "Epoch 34/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000178\n",
      "Epoch 35/50, Loss: 0.0000, Accuracy: 0.78, Learning Rate: 0.00000167\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 36/50, Loss: 13.7747, Accuracy: 0.81, Learning Rate: 0.00000156\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 37/50, Loss: 0.5979, Accuracy: 0.79, Learning Rate: 0.00000144\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 38/50, Loss: 0.0000, Accuracy: 0.79, Learning Rate: 0.00000133\n",
      "No improvement. Patience counter: 4/5\n",
      "Epoch 39/50, Loss: 0.0000, Accuracy: 0.79, Learning Rate: 0.00000122\n",
      "No improvement. Patience counter: 5/5\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from run_bert import run\n",
    "for i in range(1, 11):\n",
    "    run(i, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alan/anaconda3/envs/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30527, 768, padding_idx=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a config with the desired settings\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Load the model with the custom config\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "new_tokens = ['\\n', '(.)', '(..)', '(...)', 'xxx']\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complete = pd.read_csv(\"../data/train_complete_v3_4400.csv\")\n",
    "# train = train_complete[train_complete[\"example_index\"].apply(lambda x: len(str(x)) <= 4)]\n",
    "train = train_complete[train_complete[\"original_index\"].apply(lambda x: pd.isna(x))]\n",
    "# synthetic = train_complete[train_complete[\"original_index\"].apply(lambda x: pd.notna(x))]\n",
    "# train = pd.concat((true, synthetic.groupby('original_index').head(5)))\n",
    "lines_train = train[\"line\"].to_list()\n",
    "labels_train = train[\"label\"].to_list()\n",
    "\n",
    "test = pd.read_csv(\"../data/test_complete_v1_86.csv\")\n",
    "lines_test = test[\"line\"].to_list()\n",
    "labels_test = test[\"label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4) (86, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(lines_train, labels_train, tokenizer)\n",
    "test_dataset = TextDataset(lines_test, labels_test, tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(model, dataloader, device):\n",
    "    \"\"\"Evaluates the model on the given dataloader and returns accuracy\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            labels = batch['labels']\n",
    "\n",
    "            # Update correct predictions and totals\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            # Collect all predictions and labels for other metrics\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler with warm-up\n",
    "# cap_training_steps = 5000\n",
    "# num_epochs = cap_training_steps // len(train_dataloader)\n",
    "num_epochs = 50\n",
    "cap_training_steps = num_epochs * len(train_dataloader)\n",
    "print(\"num_epochs\", num_epochs)\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_warmup_steps = int(0.1 * cap_training_steps)  # 10% warm-up\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=cap_training_steps\n",
    ")\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade98b25ec95414f9ce3ea7155dd32a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944e39265ecd4b6faf42b21de83553b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 70.7146, Accuracy: 0.51, Learning Rate: 0.00000100\n",
      "Epoch 2/50, Loss: 65.7786, Accuracy: 0.73, Learning Rate: 0.00000200\n",
      "Epoch 3/50, Loss: 59.6564, Accuracy: 0.73, Learning Rate: 0.00000300\n",
      "Epoch 4/50, Loss: 54.4302, Accuracy: 0.69, Learning Rate: 0.00000400\n",
      "Epoch 5/50, Loss: 48.6698, Accuracy: 0.77, Learning Rate: 0.00000500\n",
      "Epoch 6/50, Loss: 40.6442, Accuracy: 0.76, Learning Rate: 0.00000489\n",
      "Epoch 7/50, Loss: 39.7658, Accuracy: 0.74, Learning Rate: 0.00000478\n",
      "Epoch 8/50, Loss: 35.8822, Accuracy: 0.80, Learning Rate: 0.00000467\n",
      "Epoch 9/50, Loss: 24.7749, Accuracy: 0.77, Learning Rate: 0.00000456\n",
      "Epoch 10/50, Loss: 22.9301, Accuracy: 0.76, Learning Rate: 0.00000444\n",
      "Epoch 11/50, Loss: 17.3018, Accuracy: 0.78, Learning Rate: 0.00000433\n",
      "Epoch 12/50, Loss: 12.8486, Accuracy: 0.77, Learning Rate: 0.00000422\n",
      "Epoch 13/50, Loss: 8.5204, Accuracy: 0.78, Learning Rate: 0.00000411\n",
      "Epoch 14/50, Loss: 7.9304, Accuracy: 0.78, Learning Rate: 0.00000400\n",
      "Epoch 15/50, Loss: 5.8050, Accuracy: 0.78, Learning Rate: 0.00000389\n",
      "Epoch 16/50, Loss: 4.3936, Accuracy: 0.79, Learning Rate: 0.00000378\n",
      "Epoch 17/50, Loss: 6.2576, Accuracy: 0.80, Learning Rate: 0.00000367\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 18/50, Loss: 1.7611, Accuracy: 0.79, Learning Rate: 0.00000356\n",
      "Epoch 19/50, Loss: 0.3140, Accuracy: 0.80, Learning Rate: 0.00000344\n",
      "Epoch 20/50, Loss: 2.8100, Accuracy: 0.83, Learning Rate: 0.00000333\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 21/50, Loss: 0.2566, Accuracy: 0.80, Learning Rate: 0.00000322\n",
      "Epoch 22/50, Loss: 0.4254, Accuracy: 0.80, Learning Rate: 0.00000311\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 23/50, Loss: 0.0416, Accuracy: 0.81, Learning Rate: 0.00000300\n",
      "Epoch 24/50, Loss: 0.0446, Accuracy: 0.79, Learning Rate: 0.00000289\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 25/50, Loss: 0.2145, Accuracy: 0.78, Learning Rate: 0.00000278\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 26/50, Loss: 0.0291, Accuracy: 0.81, Learning Rate: 0.00000267\n",
      "Epoch 27/50, Loss: 0.0330, Accuracy: 0.80, Learning Rate: 0.00000256\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 28/50, Loss: 0.0253, Accuracy: 0.81, Learning Rate: 0.00000244\n",
      "Epoch 29/50, Loss: 0.0237, Accuracy: 0.81, Learning Rate: 0.00000233\n",
      "Epoch 30/50, Loss: 0.0239, Accuracy: 0.78, Learning Rate: 0.00000222\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 31/50, Loss: 0.3503, Accuracy: 0.81, Learning Rate: 0.00000211\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 32/50, Loss: 0.0252, Accuracy: 0.78, Learning Rate: 0.00000200\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 33/50, Loss: 0.0211, Accuracy: 0.81, Learning Rate: 0.00000189\n",
      "Epoch 34/50, Loss: 0.0209, Accuracy: 0.80, Learning Rate: 0.00000178\n",
      "Epoch 35/50, Loss: 0.0169, Accuracy: 0.81, Learning Rate: 0.00000167\n",
      "Epoch 36/50, Loss: 0.0158, Accuracy: 0.83, Learning Rate: 0.00000156\n",
      "Epoch 37/50, Loss: 0.0148, Accuracy: 0.83, Learning Rate: 0.00000144\n",
      "Epoch 38/50, Loss: 0.0146, Accuracy: 0.83, Learning Rate: 0.00000133\n",
      "Epoch 39/50, Loss: 0.0140, Accuracy: 0.83, Learning Rate: 0.00000122\n",
      "Epoch 40/50, Loss: 0.1005, Accuracy: 0.81, Learning Rate: 0.00000111\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 41/50, Loss: 0.0148, Accuracy: 0.83, Learning Rate: 0.00000100\n",
      "No improvement. Patience counter: 2/5\n",
      "Epoch 42/50, Loss: 0.0206, Accuracy: 0.81, Learning Rate: 0.00000089\n",
      "No improvement. Patience counter: 3/5\n",
      "Epoch 43/50, Loss: 0.0130, Accuracy: 0.81, Learning Rate: 0.00000078\n",
      "Epoch 44/50, Loss: 0.0122, Accuracy: 0.81, Learning Rate: 0.00000067\n",
      "Epoch 45/50, Loss: 0.0117, Accuracy: 0.81, Learning Rate: 0.00000056\n",
      "Epoch 46/50, Loss: 0.0126, Accuracy: 0.81, Learning Rate: 0.00000044\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 47/50, Loss: 0.0114, Accuracy: 0.83, Learning Rate: 0.00000033\n",
      "Epoch 48/50, Loss: 0.0117, Accuracy: 0.81, Learning Rate: 0.00000022\n",
      "No improvement. Patience counter: 1/5\n",
      "Epoch 49/50, Loss: 0.0112, Accuracy: 0.81, Learning Rate: 0.00000011\n",
      "Epoch 50/50, Loss: 0.0112, Accuracy: 0.81, Learning Rate: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(cap_training_steps))\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = criterion(outputs.logits, batch['labels'])\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    epoch_accuracy = model_accuracy(model, test_dataloader, device)\n",
    "    current_lr = lr_scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}, Learning Rate: {lr_scheduler.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"models.nosync/tmp.pt\")\n",
    "        best_epoch = epoch\n",
    "        best_acc = epoch_accuracy\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    if patience_counter >= patience and best_loss < 20:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "os.rename(\"models.nosync/tmp.pt\", \"models.nosync/true_{0}_{1}epoch_{2}acc_{3}loss.pt\".format(train.shape[0], best_epoch, round(best_acc * 100), round(best_loss,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"models.nosync/true_data_49epoch_80acc.pt\"))\n",
    "model.load_state_dict(torch.load(\"models.nosync/v4_4400_33epoch_78acc_0.0loss.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test_complete_v1_149.csv\")\n",
    "lines_test = test[\"line\"].to_list()\n",
    "labels_test = test[\"label\"].to_list()\n",
    "test_dataset = TextDataset(lines_test, labels_test, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.7516778523489933\n",
      "Precision:\t 0.8157396441052185\n",
      "Recall:\t\t 0.7516778523489933\n",
      "F1 Score:\t 0.7630429841247054\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the metrics you want to compute\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Initialize lists to store all predictions and references\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Move predictions and references to CPU and convert to numpy arrays\n",
    "    predictions_cpu = predictions.cpu().numpy()\n",
    "    references_cpu = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "    # Store predictions and references\n",
    "    all_predictions.extend(predictions_cpu)\n",
    "    all_references.extend(references_cpu)\n",
    "\n",
    "# Compute all metrics at once using the full predictions\n",
    "accuracy = accuracy_metric.compute(predictions=all_predictions, references=all_references)\n",
    "precision = precision_metric.compute(predictions=all_predictions, references=all_references, average='weighted')\n",
    "recall = recall_metric.compute(predictions=all_predictions, references=all_references, average='weighted')\n",
    "f1 = f1_metric.compute(predictions=all_predictions, references=all_references, average='weighted')\n",
    "print(\"Accuracy:\\t\", accuracy['accuracy'])\n",
    "print(\"Precision:\\t\", precision['precision'])\n",
    "print(\"Recall:\\t\\t\", recall['recall'])\n",
    "print(\"F1 Score:\\t\", f1['f1'])\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_references, all_predictions)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('True', fontsize=14)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
